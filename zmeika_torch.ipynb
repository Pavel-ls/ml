{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5b4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque,namedtuple\n",
    "import time\n",
    "import random\n",
    "from itertools import count\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from random import randrange\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fd83f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x193a7813e80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74548d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732eeb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = MIN_REPLAY_MEMORY_SIZE // 10 # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '2x256'\n",
    "MIN_REWARD = -200  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 20_000\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "SHOW_PREVIEW = False\n",
    "MOMENTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a8177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "class snakeEnv():\n",
    "   \n",
    "    def __init__(self):\n",
    "        self.actions = ['up','left','right','down','none']\n",
    "        self.ACTION_SPACE_SIZE = len(self.actions)\n",
    "        self.action_space = np.array([0,1,2,3,4])\n",
    "        self.RES = 10\n",
    "        self.SIZE = 1\n",
    "        self.OBSERVATION_SPACE_VALUES = (self.RES//self.SIZE, self.RES//self.SIZE, MOMENTS) \n",
    "        self.states = []\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.length = 1\n",
    "        self.dirs = {'W': True, 'S': True, 'A': True, 'D': True, }\n",
    "        self.x, self.y = randrange(self.SIZE, self.RES - self.SIZE, self.SIZE), randrange(self.SIZE, self.RES - self.SIZE, self.SIZE)\n",
    "        self.apple = randrange(self.SIZE, self.RES - self.SIZE, self.SIZE), randrange(self.SIZE, self.RES - self.SIZE, self.SIZE)\n",
    "        self.snake = [(self.x, self.x)]\n",
    "        self.dx, self.dy = 0, 0\n",
    "        self.states = [self.getState()]*MOMENTS\n",
    "        return np.float32(np.array(self.states[-MOMENTS:]).reshape(MOMENTS,self.RES,self.RES))\n",
    "        \n",
    "        \n",
    "    def step(self, id_action):\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        init_dist = np.linalg.norm(np.array(env.snake[-1]) -np.array(env.apple))\n",
    "        \n",
    "        \n",
    "        if self.actions[id_action]=='up':\n",
    "            if self.dirs['W']:\n",
    "                self.dx, self.dy = 0, -1\n",
    "                self.dirs = {'W': True, 'S': False, 'A': True, 'D': True, }\n",
    "        elif self.actions[id_action]=='down':\n",
    "            if self.dirs['S']:\n",
    "                self.dx, self.dy = 0, 1\n",
    "                self.dirs = {'W': False, 'S': True, 'A': True, 'D': True, }\n",
    "        elif self.actions[id_action]=='left':\n",
    "            if self.dirs['A']:\n",
    "                self.dx, self.dy = -1, 0\n",
    "                self.dirs = {'W': True, 'S': True, 'A': True, 'D': False, }\n",
    "        elif self.actions[id_action]=='right':\n",
    "            if self.dirs['D']:\n",
    "                self.dx, self.dy = 1, 0\n",
    "                self.dirs = {'W': True, 'S': True, 'A': False, 'D': True, }\n",
    "        elif self.actions[id_action]=='none':\n",
    "            reward += 0.1\n",
    "                \n",
    "        self.x += self.dx * self.SIZE\n",
    "        self.y += self.dy * self.SIZE\n",
    "        \n",
    "        if self.length>1:\n",
    "            self.snake.append((self.x, self.y))\n",
    "            self.snake = self.snake[-self.length:]\n",
    "        else:\n",
    "            self.snake = [(self.x,self.y)]\n",
    "\n",
    "        \n",
    "        if self.snake[-1] == self.apple:\n",
    "            reward += 20\n",
    "            self.apple = randrange(self.SIZE, self.RES - self.SIZE, self.SIZE), randrange(self.SIZE, self.RES - self.SIZE, self.SIZE)\n",
    "            self.length += 1\n",
    "\n",
    "        if self.x < 0 or self.x > self.RES - self.SIZE or self.y < 0 or self.y > self.RES - self.SIZE or len(self.snake) != len(set(self.snake)):\n",
    "            reward += -50\n",
    "            terminated = True\n",
    "                \n",
    "        final_dist = np.linalg.norm(np.array(env.snake[-1]) -np.array(env.apple))\n",
    "                \n",
    "        observation = self.get_image()\n",
    "        reward += (init_dist - final_dist)*2\n",
    "        \n",
    "        self.states.append(np.float32(observation))\n",
    "        \n",
    "        return np.array(self.states[-MOMENTS:]).reshape(MOMENTS, self.RES,self.RES), reward, terminated\n",
    "\n",
    "\n",
    "    def getState(self):\n",
    "        return self.get_image()\n",
    "    \n",
    "    def render(self, save = False, name = '1'):\n",
    "        print()\n",
    "\n",
    "        plt.imshow(self.states[-1].reshape((self.RES,self.RES)))\n",
    "        if save:\n",
    "#             img = Image.fromarray(self.states[-1].reshape((self.RES,self.RES,3))).resize((500,500))\n",
    "#             img.save('games/'+name+'.jpg')\n",
    "            plt.savefig('games/'+name+'.jpg')\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "    def get_image(self):\n",
    "        img = Image.new(\"RGB\", (self.RES, self.RES))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        buf_size = 0 if self.SIZE == 1 else self.SIZE\n",
    "        draw.rectangle([ (self.apple[0], self.apple[1]), (buf_size +  self.apple[0], buf_size +  self.apple[1])], fill = 'grey') \n",
    "\n",
    "        if len(self.snake)>1:\n",
    "            for i, j in self.snake:\n",
    "                draw.rectangle( [(i, j), (buf_size + i, buf_size + j)], fill = 'white')\n",
    "        else:            \n",
    "            for i, j in self.snake:\n",
    "                draw.rectangle([ (self.snake[0][0], self.snake[0][1]), (buf_size +  self.snake[0][0], buf_size +  self.snake[0][1])], fill = 'white') \n",
    "\n",
    "\n",
    "\n",
    "        state_pic = np.array(img.convert(\"L\"))\n",
    "#         state_pic = np.array(img)\n",
    "\n",
    "\n",
    "        return state_pic.reshape((self.RES,self.RES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa31c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = snakeEnv()\n",
    "print(env.get_image().shape)\n",
    "env.render(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c419a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnakeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 2, out_channels  = 128, kernel_size = 3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 128, out_channels  = 256, kernel_size = 3, stride=1, padding=0)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(256,32)  \n",
    "        self.dense2 = nn.Linear(32, env.ACTION_SPACE_SIZE)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757207e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = SnakeNet().to(device)\n",
    "target_net = SnakeNet().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a182d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space[random.randint(0,env.ACTION_SPACE_SIZE-1)]]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945178cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ba3a6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_808\\221927217.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Soft update of the target network's weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_808\\116643392.py\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# In-place gradient clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Software\\anaconda\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Software\\anaconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Software\\anaconda\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             adamw(params_with_grad,\n\u001b[0m\u001b[0;32m    163\u001b[0m                   \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                   \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Software\\anaconda\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    220\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Software\\anaconda\\lib\\site-packages\\torch\\optim\\adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 20000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    episode_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, done = env.step(action.item())\n",
    "        episode_reward += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(episode_reward)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5737a79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init = env.reset()\n",
    "stp = False\n",
    "i = 0\n",
    "while not stp:\n",
    "    env.render(save= True, name = str(i))    \n",
    "    init = torch.tensor(init, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    act = policy_net(init).max(1)[1].item()\n",
    "    init, r, stp = env.step(act)\n",
    "    i += 1\n",
    "#     if i >5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34d72a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ski1l\\AppData\\Local\\Temp\\ipykernel_808\\1222942757.py:5: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread('games/'+str(i)+'.jpg')\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "with imageio.get_writer('movie.gif', mode='I') as writer:\n",
    "    for i in range(max([ int(item.replace('.jpg','')) for item in os.listdir('games')])):\n",
    "\n",
    "        image = imageio.imread('games/'+str(i)+'.jpg')\n",
    "        writer.append_data(image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982b4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "    \n",
    "#     y_pred = snakeNet.forward(batch)\n",
    "#     loss_val = mse(y_pred, y_batch)\n",
    "#     loss_val.backward()\n",
    "    \n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b656b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ep_rewards = [-200]\n",
    "\n",
    "# if not os.path.isdir('models'):\n",
    "#     os.makedirs('models')\n",
    "\n",
    "            \n",
    "# class DQNAgent:\n",
    "#     def __init__(self):\n",
    "\n",
    "#         self.model = SnakeNet()\n",
    "#         self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "#         self.loss_function = nn.MSELoss()\n",
    "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "\n",
    "\n",
    "#     def update_replay_memory(self, transition):\n",
    "#         self.replay_memory.append(transition)\n",
    "\n",
    "#     def train(self, terminal_state, step):\n",
    "\n",
    "#         if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "#             return\n",
    "# #         print(123)\n",
    "#         minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "#         current_states = np.array([transition[0] for transition in minibatch])/255\n",
    "#         current_qs_list = self.model.forward(torch.from_numpy(current_states)).detach().numpy()\n",
    "        \n",
    "#         new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
    "#         future_qs_list = self.model.forward(torch.from_numpy(new_current_states)).detach().numpy()\n",
    "\n",
    "#         X = []\n",
    "#         y = []\n",
    "\n",
    "#         for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "# #             print(2222, index)\n",
    "#             if not done:\n",
    "#                 max_future_q = np.max(future_qs_list[index])\n",
    "#                 new_q = reward + DISCOUNT * max_future_q\n",
    "#             else:\n",
    "#                 new_q = reward\n",
    "\n",
    "#             current_qs = current_qs_list[index]\n",
    "#             current_qs[action] = new_q\n",
    "\n",
    "#             X.append(current_state)\n",
    "#             y.append(current_qs)\n",
    "\n",
    "#         self.optimizer.zero_grad()\n",
    "#         X = torch.tensor(np.array(X), requires_grad = True)\n",
    "#         y_pred = self.model.forward(X)#.detach().numpy()\n",
    "#         loss_val = self.loss_function(y_pred, torch.from_numpy(np.array(y) ))\n",
    "#         loss_val.backward()      \n",
    "#         self.optimizer.step()\n",
    "        \n",
    "            \n",
    "#     def get_qs(self, state):\n",
    "#         return self.model.forward(torch.from_numpy(np.array( [state])/255))[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6661bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = DQNAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3034ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN_REWARD = -100\n",
    "# reward_hist = []\n",
    "\n",
    "\n",
    "\n",
    "# for episode in range(1, EPISODES + 1):\n",
    "#     episode_reward = 0\n",
    "#     step = 1\n",
    "\n",
    "#     current_state = env.reset()\n",
    "# #     print(episode)\n",
    "#     done = False\n",
    "#     while not done:\n",
    "\n",
    "#         if np.random.random() > epsilon:\n",
    "#             action = np.argmax(agent.get_qs(current_state.reshape(2,10,10)))\n",
    "#         else:\n",
    "#             action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "\n",
    "#         new_state, reward, done = env.step(action)\n",
    "\n",
    "#         episode_reward += reward\n",
    "#         agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "#         agent.train(done, step)\n",
    "\n",
    "#         current_state = new_state\n",
    "#         step += 1\n",
    "\n",
    "#     reward_hist.append( episode_reward )\n",
    "#     print(episode, episode_reward)\n",
    "#     if episode % 50 ==0:    \n",
    "#         MEAN_REWARD_PREV =  np.mean(reward_hist[-100:-50])\n",
    "#         MEAN_REWARD =  np.mean(reward_hist[-50:])\n",
    "# #         if MEAN_REWARD > MEAN_REWARD_PREV:\n",
    "# #             agent.model.save('models/best.model')\n",
    "#         print(episode, MEAN_REWARD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     MEAN_REWARD_PREV =  np.mean(reward_hist[-50:])\n",
    "\n",
    "#     if epsilon > MIN_EPSILON:\n",
    "#         epsilon *= EPSILON_DECAY\n",
    "#         epsilon = max(MIN_EPSILON, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007bbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
